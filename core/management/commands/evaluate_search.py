"""
Comando de Django: Evaluar Calidad de BÃºsqueda
==============================================

Ejecuta evaluaciones de mÃ©tricas de IR sobre el sistema de bÃºsqueda.
"""

from django.core.management.base import BaseCommand

from core.models import Article
from services import evaluate_search_quality, format_evaluation_report


class Command(BaseCommand):
    help = 'EvalÃºa mÃ©tricas de calidad de bÃºsqueda (Precision@K, Recall, MAP, Latencia)'
    
    def add_arguments(self, parser):
        parser.add_argument(
            '--method',
            type=str,
            default='all',
            choices=['hybrid', 'semantic', 'keyword', 'all'],
            help='MÃ©todo de bÃºsqueda a evaluar (default: all)',
        )
        parser.add_argument(
            '--queries-file',
            type=str,
            help='Archivo JSON con queries de prueba y ground truth',
        )
    
    def handle(self, *args, **options):
        method = options['method']
        queries_file = options['queries_file']
        
        self.stdout.write('=' * 80)
        self.stdout.write(self.style.HTTP_INFO('ğŸ“Š EVALUACIÃ“N DE CALIDAD DE BÃšSQUEDA'))
        self.stdout.write('=' * 80)
        
        # Verificar artÃ­culos
        num_articles = Article.objects.count()
        self.stdout.write(f'\nğŸ“‹ ArtÃ­culos en base de datos: {num_articles}')
        
        if num_articles == 0:
            self.stdout.write(self.style.ERROR('âŒ No hay artÃ­culos para evaluar'))
            return
        
        # Cargar queries de prueba
        if queries_file:
            self.stdout.write(self.style.ERROR('âŒ Carga desde archivo aÃºn no implementada'))
            return
        
        # Crear queries de prueba bÃ¡sicas
        test_queries = self._create_test_queries()
        
        if not test_queries:
            self.stdout.write(self.style.WARNING('âš ï¸  No se pudieron crear queries de prueba'))
            return
        
        self.stdout.write(f'âœ… Queries de prueba: {len(test_queries)}')
        
        # Evaluar mÃ©todos
        methods_to_eval = ['hybrid', 'semantic', 'keyword'] if method == 'all' else [method]
        
        results = {}
        for eval_method in methods_to_eval:
            self.stdout.write(f'\n{"=" * 80}')
            self.stdout.write(f'Evaluando: {eval_method.upper()}')
            self.stdout.write(f'{"=" * 80}')
            
            try:
                evaluation = evaluate_search_quality(
                    test_queries=test_queries,
                    method=eval_method,
                    k_values=[1, 3, 5, 10]
                )
                
                results[eval_method] = evaluation
                
                # Mostrar reporte
                report = format_evaluation_report(evaluation)
                self.stdout.write(report)
                
            except Exception as e:
                self.stdout.write(self.style.ERROR(f'âŒ Error: {e}'))
        
        # ComparaciÃ³n final si se evaluaron mÃºltiples mÃ©todos
        if len(results) > 1:
            self._show_comparison(results)
    
    def _create_test_queries(self):
        """Crea queries de prueba automÃ¡ticamente basadas en artÃ­culos existentes."""
        all_articles = list(Article.objects.all().values('id', 'title', 'snippet'))
        test_queries = []
        
        # Query patterns
        patterns = [
            (['transparencia', 'corrupciÃ³n'], 'transparencia y corrupciÃ³n'),
            (['educaciÃ³n', 'educacion'], 'educaciÃ³n'),
            (['salud'], 'salud mental'),
            (['delito', 'penal', 'cibernÃ©tico', 'cibernetico'], 'delitos informÃ¡ticos'),
            (['ambiente', 'ambiental', 'plÃ¡stico', 'plastico'], 'medio ambiente'),
        ]
        
        for keywords, query_text in patterns:
            relevant_ids = {
                art['id'] for art in all_articles 
                if any(kw in art['title'].lower() for kw in keywords) or
                   (art['snippet'] and any(kw in art['snippet'].lower() for kw in keywords))
            }
            
            if relevant_ids:
                test_queries.append({
                    'query': query_text,
                    'relevant_ids': relevant_ids
                })
        
        return test_queries
    
    def _show_comparison(self, results):
        """Muestra comparaciÃ³n entre mÃ©todos."""
        self.stdout.write('\n' + '=' * 80)
        self.stdout.write(self.style.HTTP_INFO('ğŸ“Š COMPARACIÃ“N DE MÃ‰TODOS'))
        self.stdout.write('=' * 80)
        
        # Precision@1
        self.stdout.write('\nPrecision@1:')
        for method, eval_data in results.items():
            p1 = eval_data['precision_at_k'].get(1, 0.0)
            status = 'âœ…' if p1 >= 0.95 else 'âš ï¸'
            self.stdout.write(f'  {method:10s}: {p1:.3f} ({p1*100:.1f}%) {status}')
        
        # Latencia
        self.stdout.write('\nLatencia Media:')
        for method, eval_data in results.items():
            mean_lat = eval_data['latency_ms']['mean']
            status = 'âœ…' if mean_lat < 200 else 'âš ï¸'
            self.stdout.write(f'  {method:10s}: {mean_lat:6.1f} ms {status}')
        
        # MAP
        self.stdout.write('\nMAP:')
        for method, eval_data in results.items():
            map_score = eval_data['map']
            self.stdout.write(f'  {method:10s}: {map_score:.3f}')
        
        # Mejor mÃ©todo
        self.stdout.write('\nğŸ† MEJOR POR MÃ‰TRICA:')
        best_p1 = max(results.items(), key=lambda x: x[1]['precision_at_k'].get(1, 0.0))
        self.stdout.write(f'  Precision@1: {best_p1[0]}')
        
        best_latency = min(results.items(), key=lambda x: x[1]['latency_ms']['mean'])
        self.stdout.write(f'  Latencia:    {best_latency[0]}')
        
        best_map = max(results.items(), key=lambda x: x[1]['map'])
        self.stdout.write(f'  MAP:         {best_map[0]}')
        
        self.stdout.write('\n' + '=' * 80)
