import feedparser
import requests
from bs4 import BeautifulSoup
from dateutil import parser
from .models import NewsSource, Article, NewsPreset
from django.utils import timezone
import google.generativeai as genai
import unicodedata
import time
import datetime
import difflib
import urllib3

# Desactivar las advertencias molestas de seguridad SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- TU CLAVE DE GOOGLE ---
GOOGLE_API_KEY = "AIzaSyBl561zpgqQlziXDPvy6yvrBHJ_-GQ7_-M"

genai.configure(api_key=GOOGLE_API_KEY)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'es-ES,es;q=0.9',
    'Referer': 'https://www.google.com/'
}

# --- 1. COMPARADOR VISUAL (COLORES) ---
def generate_diff_html(text1, text2):
    lines1 = text1.splitlines()
    lines2 = text2.splitlines()
    d = difflib.HtmlDiff()
    html_diff = d.make_table(lines1, lines2, fromdesc='Versi√≥n A', todesc='Versi√≥n B', context=True, numlines=5)
    html_diff = html_diff.replace('nowrap="nowrap"', '')
    return html_diff

# --- 2. COMPARADOR INTELIGENTE (IA) ---
def analyze_legal_diff(text_old, text_new):
    try:
        available_model = None
        try:
            for m in genai.list_models():
                if 'generateContent' in m.supported_generation_methods:
                    available_model = m.name
                    break 
        except: pass
        
        if not available_model: available_model = 'models/gemini-1.5-flash'
        
        model = genai.GenerativeModel(available_model)
        
        prompt = f"""
        Act√∫a como un Abogado Corporativo Senior. Compara los textos y genera un Resumen Ejecutivo.
        
        REGLAS DE FORMATO ESTRICTAS (NO LAS ROMPAS):
        1. NO saludes, NO des introducciones.
        2. Tu respuesta debe ser EXCLUSIVAMENTE c√≥digo HTML.
        3. Usa la etiqueta <h3> para el t√≠tulo principal.
        4. Usa la etiqueta <ul class="list-disc pl-5 space-y-2"> para la lista de puntos.
        5. Cada punto debe ser un <li> con <strong>Concepto:</strong>.

        --- VERSI√ìN ORIGINAL ---
        {text_old}

        --- VERSI√ìN NUEVA ---
        {text_new}
        """
        
        response = model.generate_content(prompt)
        clean_text = response.text.replace("```html", "").replace("```", "")
        return clean_text

    except Exception as e:
        return f"<p class='text-red-500'><strong>Error analizando con IA:</strong> {str(e)}</p>"

# --- 3. FUNCIONES DEL ROBOT DE NOTICIAS ---
def normalize_text(text):
    if not text: return ""
    text = text.lower()
    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')

def calculate_relevance_score(text, keywords, fields_to_analyze="title,description"):
    """
    Calcula un score de relevancia (0-100) basado en coincidencias de keywords.
    
    Args:
        text: Texto a analizar
        keywords: Lista de palabras clave
        fields_to_analyze: Campos considerados (no usado en scoring simple)
    
    Retorna: Score entre 0-100
    """
    if not text or not keywords:
        return 0.0
    
    text_normalized = normalize_text(text)
    text_words = set(text_normalized.split())
    
    total_keywords = len(keywords)
    if total_keywords == 0:
        return 0.0
    
    # Contar coincidencias
    matches = 0
    for keyword in keywords:
        keyword_normalized = normalize_text(keyword)
        
        # Coincidencia exacta de frase
        if keyword_normalized in text_normalized:
            matches += 2  # Peso doble para coincidencias exactas
        # Coincidencia de palabra individual
        elif keyword_normalized in text_words:
            matches += 1
    
    # Calcular score como porcentaje
    # M√°ximo posible: total_keywords * 2 (todas coincidencias exactas)
    max_possible = total_keywords * 2
    score = (matches / max_possible) * 100
    
    return min(100.0, score)  # Limitar a 100

def scrape_full_text(url):
    try:
        time.sleep(0.3) 
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.encoding = 'utf-8'
        if response.status_code != 200: return None
        soup = BeautifulSoup(response.text, 'html.parser')
        full_text = ""
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = p.get_text().strip()
            if len(text) > 40 and "Copyright" not in text and "Suscr√≠bete" not in text:
                full_text += f"<p>{text}</p>"
        return full_text if len(full_text) > 100 else None
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return None

def get_image_from_entry(entry):
    if 'media_content' in entry:
        for media in entry.media_content:
            if 'image' in media.get('type', '') or 'jpg' in media.get('url', ''): return media['url']
    if 'enclosures' in entry:
        for enclosure in entry.enclosures:
            if 'image' in enclosure.get('type', ''): return enclosure['href']
    if 'media_thumbnail' in entry: return entry.media_thumbnail[0]['url']
    return None

# --- FUNCIONES DE SINCRONIZACI√ìN RSS ---

def get_active_sources():
    """Obtiene todas las fuentes RSS activas."""
    return NewsSource.objects.filter(is_active=True)

def get_active_presets():
    """Obtiene todos los presets de filtrado activos."""
    return NewsPreset.objects.filter(is_active=True)

def check_match(text, presets):
    """
    Verifica si un texto coincide con los keywords de alg√∫n preset usando scoring.
    Retorna: (match: bool, preset_name: str, score: float)
    """
    if not presets:
        return True, None, 100.0
    
    text_normalized = normalize_text(text)
    best_score = 0.0
    best_preset = None
    
    for preset in presets:
        # Obtener keywords y threshold
        keywords = [k.strip() for k in preset.keywords.split(',') if k.strip()]
        threshold = preset.threshold if hasattr(preset, 'threshold') else 30
        fields = preset.fields_to_analyze if hasattr(preset, 'fields_to_analyze') else "title,description"
        
        # Calcular score
        score = calculate_relevance_score(text, keywords, fields)
        
        # Guardar mejor score
        if score > best_score:
            best_score = score
            best_preset = preset.name
    
    # Determinar si pasa el filtro
    # Usar el threshold del mejor preset
    threshold_to_use = 30  # Default
    for preset in presets:
        if preset.name == best_preset:
            threshold_to_use = preset.threshold if hasattr(preset, 'threshold') else 30
            break
    
    match = best_score >= threshold_to_use
    
    return match, best_preset, best_score

def article_exists(link):
    """Verifica si un art√≠culo ya existe en la base de datos por URL."""
    return Article.objects.filter(link=link).exists()

def extract_article_date(entry):
    """Extrae la fecha de publicaci√≥n de una entrada RSS."""
    if hasattr(entry, 'published'):
        try:
            return parser.parse(entry.published)
        except:
            pass
    return timezone.now()

def create_article_from_entry(source, entry, snippet, image_url, preset_name=None, score=0.0):
    """
    Crea un art√≠culo en la base de datos desde una entrada RSS.
    Retorna el art√≠culo creado.
    """
    article = Article.objects.create(
        source=source,
        title=entry.title,
        link=entry.link,
        published_at=extract_article_date(entry),
        snippet=snippet,
        image_url=image_url,
        relevance_score=score
    )
    
    print(f"‚úÖ NUEVA: '{entry.title[:50]}...' (Preset: {preset_name or 'N/A'}, Score: {score:.1f})")
    return article

def process_rss_entry(source, entry, active_presets):
    """
    Procesa una entrada RSS individual con filtrado en dos pasadas.
    
    Primera pasada: Filtra por t√≠tulo y snippet del feed (r√°pido)
    Segunda pasada: Descarga contenido y confirma relevancia (exhaustivo)
    
    Retorna: True si se cre√≥ un art√≠culo, False si no.
    """
    link = entry.link
    
    # Evitar duplicados
    if article_exists(link):
        return False
    
    title = entry.title
    feed_snippet = entry.summary if hasattr(entry, 'summary') else ""
    
    # PRIMERA PASADA: Filtro r√°pido en t√≠tulo + snippet del feed
    # Esto ahorra descargas innecesarias
    preliminary_text = title + " " + feed_snippet
    match_preliminary, preset_name, score_preliminary = check_match(preliminary_text, active_presets)
    
    if not match_preliminary:
        # No pasa filtro preliminar, descartamos sin descargar
        return False
    
    print(f"üìå Filtro preliminar OK: '{title[:50]}...' (Preset: {preset_name}, Score: {score_preliminary:.1f})")
    
    # SEGUNDA PASADA: Descargar contenido completo y confirmar
    full_content = scrape_full_text(link)
    if not full_content:
        # Si no se pudo scraping completo, usar snippet del feed
        full_content = feed_snippet if feed_snippet else title
    
    # Confirmar relevancia con contenido completo
    match_final, final_preset, final_score = check_match(title + " " + full_content, active_presets)
    
    if not match_final:
        # Pas√≥ preliminar pero no confirmaci√≥n (falso positivo)
        print(f"  ‚ö†Ô∏è  Descartado tras an√°lisis completo (falso positivo)")
        return False
    
    # Obtener imagen
    image_url = get_image_from_entry(entry)
    
    # Crear art√≠culo confirmado como relevante con score final
    create_article_from_entry(source, entry, full_content, image_url, final_preset, final_score)
    
    return True

def sync_rss_source(source, active_presets, max_entries=10):
    """
    Sincroniza una fuente RSS individual.
    Retorna: cantidad de art√≠culos nuevos creados.
    """
    try:
        feed = feedparser.parse(source.url, request_headers=HEADERS)
        new_count = 0
        
        for entry in feed.entries[:max_entries]:
            if process_rss_entry(source, entry, active_presets):
                new_count += 1
        
        return new_count
        
    except Exception as e:
        print(f"‚ùå Error en fuente {source.name}: {e}")
        return 0

def clean_invalid_articles():
    """
    Elimina art√≠culos que ya no coinciden con los presets activos.
    Retorna: cantidad de art√≠culos eliminados.
    """
    print("--- RE-VALIDANDO ART√çCULOS EXISTENTES ---")
    
    active_presets = get_active_presets()
    articles = Article.objects.all()
    deleted_count = 0
    
    for article in articles:
        full_content = article.snippet or ""
        match, _, score = check_match(article.title + " " + full_content, active_presets)
        
        if not match:
            article.delete()
            deleted_count += 1
        else:
            # Actualizar score si cambi√≥
            if article.relevance_score != score:
                article.relevance_score = score
                article.save(update_fields=['relevance_score'])
    
    print(f"--- LIMPIEZA: {deleted_count} art√≠culos eliminados ---\n")
    return deleted_count

def sync_all_rss_sources(max_entries=10, clean_first=True):
    """
    Sincroniza todas las fuentes RSS activas.
    
    Args:
        max_entries: N√∫mero m√°ximo de entradas a procesar por fuente
        clean_first: Si True, limpia art√≠culos inv√°lidos antes de sincronizar
    
    Retorna: cantidad total de art√≠culos nuevos.
    """
    # Limpiar art√≠culos obsoletos
    if clean_first:
        clean_invalid_articles()
    
    # Obtener fuentes y presets activos
    sources = get_active_sources()
    active_presets = get_active_presets()
    
    if not sources.exists():
        print("‚ö†Ô∏è No hay fuentes RSS activas")
        return 0
    
    print(f"--- SINCRONIZANDO {sources.count()} FUENTES RSS ---")
    
    total_new = 0
    for source in sources:
        new_count = sync_rss_source(source, active_presets, max_entries)
        total_new += new_count
    
    print(f"\n--- TOTAL: {total_new} art√≠culos nuevos ---")
    return total_new

# Alias para compatibilidad con c√≥digo existente
def fetch_latest_news():
    """Funci√≥n legacy - mantiene compatibilidad con c√≥digo existente."""
    return sync_all_rss_sources(max_entries=10, clean_first=True)

def sync_database_with_filters():
    """Funci√≥n legacy - mantiene compatibilidad con c√≥digo existente."""
    return clean_invalid_articles()

def calculate_content_hash(text):
    """Calcula un hash MD5 del contenido para validaci√≥n de cach√©."""
    import hashlib
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def chunk_text(text, max_words=800):
    """
    Divide un texto largo en fragmentos manejables.
    
    Args:
        text: Texto a dividir
        max_words: M√°ximo de palabras por fragmento
    
    Retorna: Lista de fragmentos de texto
    """
    words = text.split()
    chunks = []
    
    for i in range(0, len(words), max_words):
        chunk = ' '.join(words[i:i + max_words])
        chunks.append(chunk)
    
    return chunks

def get_available_gemini_model():
    """Obtiene un modelo Gemini disponible."""
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                return m.name
    except:
        pass
    return 'models/gemini-1.5-flash'

def summarize_text_chunk(model, chunk, is_final=False):
    """
    Resume un fragmento de texto individual.
    
    Args:
        model: Modelo Gemini configurado
        chunk: Texto del fragmento
        is_final: Si es el resumen final (combina chunks previos)
    
    Retorna: Texto del resumen
    """
    if is_final:
        # Resumen final que combina mini-res√∫menes
        prompt = """
Resume el siguiente contenido en 3-5 puntos clave siguiendo el formato de las 5W del periodismo.
Usa HTML con vi√±etas (<ul><li>). Estructura:

1. QU√â/QUI√âN: El hecho principal y protagonistas
2. D√ìNDE/CU√ÅNDO: Ubicaci√≥n y tiempo del evento
3. POR QU√â/C√ìMO: Contexto, causas o m√©todo
4. IMPACTO: Consecuencias o importancia
5. QU√â SIGUE (si aplica): Pr√≥ximos pasos o situaci√≥n futura

S√© conciso, objetivo y claro. Usa espa√±ol neutral.

Texto a resumir:
""" + chunk
    else:
        # Resumen de un fragmento individual
        prompt = f"""
Resume los puntos principales del siguiente fragmento de noticia de forma concisa.
Enf√≥cate en hechos concretos, protagonistas y datos relevantes.

Fragmento:
{chunk}
"""
    
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Error resumiendo fragmento: {str(e)}"

def generate_ai_summary(article_id):
    """
    Genera un resumen IA para un art√≠culo usando chunking para textos largos.
    Implementa formato 5W (qui√©n, qu√©, d√≥nde, cu√°ndo, por qu√©).
    Valida cach√© de res√∫menes usando content_hash.
    """
    try:
        article = Article.objects.get(id=article_id)
        
        # Obtener contenido a resumir
        content = article.snippet or ""
        if not content or len(content) < 100:
            article.ai_summary = "‚ö†Ô∏è <strong>Contenido insuficiente</strong> para generar resumen."
            article.save()
            return False
        
        # Calcular hash del contenido actual
        current_hash = calculate_content_hash(content)
        
        # Verificar si ya existe un resumen v√°lido
        if article.ai_summary and article.content_hash == current_hash:
            print(f"üìã Resumen en cach√© v√°lido para: {article.title[:50]}...")
            return True  # Resumen existente y v√°lido, no regenerar
        
        # Si el hash no coincide o no hay resumen, generamos uno nuevo
        if article.ai_summary and article.content_hash != current_hash:
            print(f"üîÑ Contenido modificado, regenerando resumen...")
        
        # Obtener modelo disponible
        model_name = get_available_gemini_model()
        model = genai.GenerativeModel(model_name)
        
        # Determinar si necesitamos chunking
        word_count = len(content.split())
        
        if word_count <= 800:
            # Texto corto - resumen directo
            summary = summarize_text_chunk(model, content, is_final=True)
        else:
            # Texto largo - dividir en chunks, resumir cada uno, luego combinar
            print(f"üìÑ Art√≠culo largo ({word_count} palabras) - usando chunking")
            
            chunks = chunk_text(content, max_words=800)
            chunk_summaries = []
            
            for i, chunk in enumerate(chunks):
                print(f"  Resumiendo fragmento {i+1}/{len(chunks)}...")
                chunk_summary = summarize_text_chunk(model, chunk, is_final=False)
                chunk_summaries.append(chunk_summary)
                time.sleep(0.5)  # Peque√±a pausa entre chunks
            
            # Combinar res√∫menes de chunks y hacer resumen final
            combined = "\n\n".join(chunk_summaries)
            print(f"  Generando resumen final...")
            summary = summarize_text_chunk(model, combined, is_final=True)
        
        # Guardar resumen y hash
        article.ai_summary = summary
        article.content_hash = current_hash
        article.save()
        
        print(f"‚úÖ Resumen generado para: {article.title[:50]}...")
        return True
        
    except Exception as e:
        print(f"‚ùå Error generando resumen: {str(e)}")
        article.ai_summary = f"‚ö†Ô∏è <strong>ERROR T√âCNICO:</strong> {str(e)}"
        article.save()
        return False

# --- 4. ROBOT DE SUTRA (MODO INSEGURO ACTIVADO) ---
def check_sutra_status(measure_id):
    """
    Verifica estado en SUTRA ignorando errores SSL.
    """
    clean_id = measure_id.lower().replace("p. de la c.", "pc").replace(" ", "").replace(".", "")
    base_url = f"https://sutra.oslpr.org/osl/es/medidas/{clean_id}"
    
    try:
        # verify=False ES LA CLAVE PARA ARREGLAR TU ERROR
        response = requests.get(base_url, headers=HEADERS, timeout=10, verify=False)
        
        if response.status_code == 200:
            return True, f"Conexi√≥n exitosa: {base_url}"
        else:
            return False, f"No encontrado (Status {response.status_code})"
    except Exception as e:
        return False, f"Error de conexi√≥n: {str(e)}"