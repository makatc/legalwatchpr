"""
Script de Evaluaci√≥n de M√©tricas de B√∫squeda
============================================

Ejecuta evaluaciones de calidad de b√∫squeda usando m√©tricas de IR.
"""

import os
import sys
import django
import pytest
from pathlib import Path
from datetime import datetime, timedelta

# Configure Django: ensure project root is on sys.path (robust)
_env_root = os.getenv('LW_PROJECT_ROOT')
if _env_root:
    _root = Path(_env_root).resolve()
else:
    _root = Path(__file__).resolve().parent
    while not (_root / 'manage.py').exists() and _root.parent != _root:
        _root = _root.parent
    _root = _root

if str(_root) not in sys.path:
    sys.path.insert(0, str(_root))

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')
try:
    django.setup()
except RuntimeError:
    # django.setup() may have been called already during collection; ignore re-entrant setup
    pass

# Allow DB access for these evaluation scripts
pytestmark = pytest.mark.django_db

from services import (
    evaluate_search_quality, 
    format_evaluation_report,
    SearchMetrics
)
from core.models import Article

def _run_metrics_tests():
    print("=" * 80)
    print("üìä EVALUACI√ìN DE M√âTRICAS DE B√öSQUEDA")
    print("=" * 80)

    # Verificar que hay art√≠culos
    num_articles = Article.objects.count()
    print(f"\nüìã Art√≠culos en base de datos: {num_articles}")

    if num_articles == 0:
        print("‚ö†Ô∏è No hay art√≠culos en la BD ‚Äî creando art√≠culos de prueba para la evaluaci√≥n...")
        # Crear fuente y art√≠culos de prueba (mismo dataset que test_hybrid_search)
        from core.models import NewsSource
        test_source, _ = NewsSource.objects.get_or_create(
            name="Fuente de Prueba",
            defaults={'url': 'https://ejemplo.pr/feed', 'is_active': True}
        )

        test_articles = [
            {
                'title': 'Ley de Transparencia y Acceso a la Informaci√≥n P√∫blica',
                'snippet': 'El Senado aprob√≥ una nueva ley de transparencia que requiere a todas las agencias gubernamentales publicar sus presupuestos en l√≠nea. La medida busca combatir la corrupci√≥n y mejorar la rendici√≥n de cuentas.',
                'url': 'https://ejemplo.pr/transparencia-2026',
            },
            {
                'title': 'Proyecto de Ley sobre Educaci√≥n Virtual en Puerto Rico',
                'snippet': 'La C√°mara de Representantes considera legislaci√≥n para regular la educaci√≥n virtual. El proyecto establece est√°ndares de calidad y accesibilidad para plataformas educativas en l√≠nea.',
                'url': 'https://ejemplo.pr/educacion-virtual',
            },
            {
                'title': 'Reforma al C√≥digo Penal: Nuevas Penas por Delitos Cibern√©ticos',
                'snippet': 'Se propone endurecer las penas para delitos de fraude electr√≥nico y robo de identidad digital. La reforma incluye sanciones de hasta 15 a√±os de prisi√≥n para casos graves de cibercrimen.',
                'url': 'https://ejemplo.pr/reforma-penal',
            },
            {
                'title': 'Cambios en la Ley de Salud Mental: Mayor Acceso a Servicios',
                'snippet': 'Nueva legislaci√≥n ampl√≠a la cobertura de servicios de salud mental en el plan de salud gubernamental. Se crean centros comunitarios de atenci√≥n psicol√≥gica en 10 municipios.',
                'url': 'https://ejemplo.pr/salud-mental',
            },
            {
                'title': 'Medida Ambiental: Prohibici√≥n de Pl√°sticos de Un Solo Uso',
                'snippet': 'A partir de julio de 2026, entra en vigor la prohibici√≥n de pl√°sticos desechables en comercios. La ley promueve el uso de materiales biodegradables y reutilizables para reducir la contaminaci√≥n.',
                'url': 'https://ejemplo.pr/plasticos-prohibidos',
            },
        ]

        for art_data in test_articles:
            Article.objects.get_or_create(
                link=art_data['url'],
                defaults={
                    'title': art_data['title'],
                    'snippet': art_data['snippet'],
                    'source': test_source,
                    'published_at': datetime.now() - timedelta(days=1),
                }
            )
        num_articles = Article.objects.count()
        print(f"‚úÖ Art√≠culos creados: {num_articles}")

    # Obtener algunos IDs de art√≠culos para ground truth
    all_articles = list(Article.objects.all().values('id', 'title', 'snippet'))

    print(f"\nüìù Art√≠culos disponibles:")
    for i, art in enumerate(all_articles[:10], 1):
        print(f"  {i}. [{art['id']}] {art['title'][:60]}...")

    # Crear queries de prueba con ground truth conocido
    # Basado en los art√≠culos de prueba creados anteriormente
    test_queries = []

    # Query 1: transparencia
    transparencia_ids = {
        art['id'] for art in all_articles 
        if 'transparencia' in art['title'].lower() or 
           (art['snippet'] and 'transparencia' in art['snippet'].lower())
    }
    if transparencia_ids:
        test_queries.append({
            'query': 'transparencia y corrupci√≥n',
            'relevant_ids': transparencia_ids
        })

    # Query 2: educaci√≥n
    educacion_ids = {
        art['id'] for art in all_articles 
        if 'educaci√≥n' in art['title'].lower() or 'educacion' in art['title'].lower() or
           (art['snippet'] and ('educaci√≥n' in art['snippet'].lower() or 'educacion' in art['snippet'].lower()))
    }
    if educacion_ids:
        test_queries.append({
            'query': 'educaci√≥n',
            'relevant_ids': educacion_ids
        })

    # Query 3: salud
    salud_ids = {
        art['id'] for art in all_articles 
        if 'salud' in art['title'].lower() or 
           (art['snippet'] and 'salud' in art['snippet'].lower())
    }
    if salud_ids:
        test_queries.append({
            'query': 'salud mental',
            'relevant_ids': salud_ids
        })

    # Query 4: delitos
    delitos_ids = {
        art['id'] for art in all_articles 
        if any(term in art['title'].lower() for term in ['delito', 'penal', 'cibern√©tico', 'cibernetico']) or
           (art['snippet'] and any(term in art['snippet'].lower() for term in ['delito', 'penal', 'cibern√©tico', 'cibernetico']))
    }
    if delitos_ids:
        test_queries.append({
            'query': 'delitos inform√°ticos',
            'relevant_ids': delitos_ids
        })

    # Query 5: ambiente
    ambiente_ids = {
        art['id'] for art in all_articles 
        if any(term in art['title'].lower() for term in ['ambiente', 'ambiental', 'pl√°stico', 'plastico']) or
           (art['snippet'] and any(term in art['snippet'].lower() for term in ['ambiente', 'ambiental', 'pl√°stico', 'plastico']))
    }
    if ambiente_ids:
        test_queries.append({
            'query': 'medio ambiente',
            'relevant_ids': ambiente_ids
        })

    if not test_queries:
        print("\n‚ö†Ô∏è  No se pudieron crear queries de prueba. Verifica los datos.")
        sys.exit(1)

    print(f"\n‚úÖ Queries de prueba creadas: {len(test_queries)}")
    for i, q in enumerate(test_queries, 1):
        print(f"  {i}. '{q['query']}' ‚Üí {len(q['relevant_ids'])} documentos relevantes")

    # Evaluar cada m√©todo
    print("\n" + "=" * 80)
    print("üîç EJECUTANDO EVALUACIONES")
    print("=" * 80)

    methods = ['hybrid', 'semantic', 'keyword']
    k_values = [1, 3, 5, 10]

    results = {}

    for method in methods:
        print(f"\n{'=' * 80}")
        print(f"Evaluando m√©todo: {method.upper()}")
        print(f"{'=' * 80}")
        
        try:
            evaluation = evaluate_search_quality(
                test_queries=test_queries,
                method=method,
                k_values=k_values
            )
            
            results[method] = evaluation
            
            # Mostrar reporte
            report = format_evaluation_report(evaluation)
            print(report)
            
        except Exception as e:
            print(f"‚ùå Error evaluando {method}: {e}")
            import traceback
            traceback.print_exc()

    # Comparaci√≥n final
    print("\n" + "=" * 80)
    print("üìä COMPARACI√ìN DE M√âTODOS")
    print("=" * 80)

    if results:
        print("\nPrecision@1:")
        for method, eval_data in results.items():
            p1 = eval_data['precision_at_k'].get(1, 0.0)
            print(f"  {method:10s}: {p1:.3f} ({p1*100:.1f}%)")
        
        print("\nMAP (Mean Average Precision):")
        for method, eval_data in results.items():
            map_score = eval_data['map']
            print(f"  {method:10s}: {map_score:.3f}")
        
        print("\nLatencia Media:")
        for method, eval_data in results.items():
            mean_lat = eval_data['latency_ms']['mean']
            status = "‚úÖ" if mean_lat < 200 else "‚ö†Ô∏è"
            print(f"  {method:10s}: {mean_lat:6.1f} ms {status}")
        
        print("\nRecall:")
        for method, eval_data in results.items():
            recall = eval_data['recall']
            print(f"  {method:10s}: {recall:.3f} ({recall*100:.1f}%)")
        
        # Mejor m√©todo por m√©trica
        print("\nüèÜ MEJORES M√âTODOS POR M√âTRICA:")
        
        best_p1 = max(results.items(), key=lambda x: x[1]['precision_at_k'].get(1, 0.0))
        print(f"  Precision@1: {best_p1[0]} ({best_p1[1]['precision_at_k'][1]:.1%})")
        
        best_recall = max(results.items(), key=lambda x: x[1]['recall'])
        print(f"  Recall:      {best_recall[0]} ({best_recall[1]['recall']:.1%})")
        
        best_latency = min(results.items(), key=lambda x: x[1]['latency_ms']['mean'])
        print(f"  Latencia:    {best_latency[0]} ({best_latency[1]['latency_ms']['mean']:.1f} ms)")
        
        best_map = max(results.items(), key=lambda x: x[1]['map'])
        print(f"  MAP:         {best_map[0]} ({best_map[1]['map']:.3f})")

    print("\n" + "=" * 80)
    print("‚úÖ EVALUACI√ìN COMPLETADA")
    print("=" * 80)

    # Recomendaciones
    print("\nüí° RECOMENDACIONES:")
    print("-" * 80)

    if results:
        hybrid_eval = results.get('hybrid')
        if hybrid_eval:
            p1 = hybrid_eval['precision_at_k'].get(1, 0.0)
            lat = hybrid_eval['latency_ms']['mean']
            recall = hybrid_eval['recall']
            
            if p1 >= 0.95:
                print("‚úÖ Precision@1 excelente para b√∫squedas legales espec√≠ficas")
            else:
                print("‚ö†Ô∏è  Considera ajustar pesos RRF o mejorar embeddings para mayor Precision@1")
            
            if lat < 200:
                print("‚úÖ Latencia dentro del objetivo (<200ms)")
            else:
                print("‚ùå Latencia alta - considera:")
                print("   ‚Ä¢ Crear √≠ndice HNSW para embeddings")
                print("   ‚Ä¢ Optimizar par√°metros de b√∫squeda")
                print("   ‚Ä¢ Reducir top_k_candidates en RRF")
            
            if recall >= 0.80:
                print("‚úÖ Recall alto - el sistema encuentra la mayor√≠a de documentos relevantes")
            else:
                print("‚ö†Ô∏è  Recall bajo - el sistema puede estar perdiendo documentos relevantes")
                print("   ‚Ä¢ Aumentar top_k_candidates en b√∫squeda")
                print("   ‚Ä¢ Revisar calidad de embeddings")
                print("   ‚Ä¢ Verificar cobertura de search_vector")

    print("\n" + "=" * 80)


@pytest.mark.django_db
def test_metrics_runner():
    """Run metrics evaluation script under pytest with DB access."""
    _run_metrics_tests()
