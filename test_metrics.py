"""
Script de Evaluaci√≥n de M√©tricas de B√∫squeda
============================================

Ejecuta evaluaciones de calidad de b√∫squeda usando m√©tricas de IR.
"""

import os
import sys
import django

# Configurar Django
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')
django.setup()

from services import (
    evaluate_search_quality, 
    format_evaluation_report,
    SearchMetrics
)
from core.models import Article

print("=" * 80)
print("üìä EVALUACI√ìN DE M√âTRICAS DE B√öSQUEDA")
print("=" * 80)

# Verificar que hay art√≠culos
num_articles = Article.objects.count()
print(f"\nüìã Art√≠culos en base de datos: {num_articles}")

if num_articles == 0:
    print("‚ùå No hay art√≠culos para evaluar. Ejecuta primero test_hybrid_search.py")
    sys.exit(1)

# Obtener algunos IDs de art√≠culos para ground truth
all_articles = list(Article.objects.all().values('id', 'title', 'snippet'))

print(f"\nüìù Art√≠culos disponibles:")
for i, art in enumerate(all_articles[:10], 1):
    print(f"  {i}. [{art['id']}] {art['title'][:60]}...")

# Crear queries de prueba con ground truth conocido
# Basado en los art√≠culos de prueba creados anteriormente
test_queries = []

# Query 1: transparencia
transparencia_ids = {
    art['id'] for art in all_articles 
    if 'transparencia' in art['title'].lower() or 
       (art['snippet'] and 'transparencia' in art['snippet'].lower())
}
if transparencia_ids:
    test_queries.append({
        'query': 'transparencia y corrupci√≥n',
        'relevant_ids': transparencia_ids
    })

# Query 2: educaci√≥n
educacion_ids = {
    art['id'] for art in all_articles 
    if 'educaci√≥n' in art['title'].lower() or 'educacion' in art['title'].lower() or
       (art['snippet'] and ('educaci√≥n' in art['snippet'].lower() or 'educacion' in art['snippet'].lower()))
}
if educacion_ids:
    test_queries.append({
        'query': 'educaci√≥n',
        'relevant_ids': educacion_ids
    })

# Query 3: salud
salud_ids = {
    art['id'] for art in all_articles 
    if 'salud' in art['title'].lower() or 
       (art['snippet'] and 'salud' in art['snippet'].lower())
}
if salud_ids:
    test_queries.append({
        'query': 'salud mental',
        'relevant_ids': salud_ids
    })

# Query 4: delitos
delitos_ids = {
    art['id'] for art in all_articles 
    if any(term in art['title'].lower() for term in ['delito', 'penal', 'cibern√©tico', 'cibernetico']) or
       (art['snippet'] and any(term in art['snippet'].lower() for term in ['delito', 'penal', 'cibern√©tico', 'cibernetico']))
}
if delitos_ids:
    test_queries.append({
        'query': 'delitos inform√°ticos',
        'relevant_ids': delitos_ids
    })

# Query 5: ambiente
ambiente_ids = {
    art['id'] for art in all_articles 
    if any(term in art['title'].lower() for term in ['ambiente', 'ambiental', 'pl√°stico', 'plastico']) or
       (art['snippet'] and any(term in art['snippet'].lower() for term in ['ambiente', 'ambiental', 'pl√°stico', 'plastico']))
}
if ambiente_ids:
    test_queries.append({
        'query': 'medio ambiente',
        'relevant_ids': ambiente_ids
    })

if not test_queries:
    print("\n‚ö†Ô∏è  No se pudieron crear queries de prueba. Verifica los datos.")
    sys.exit(1)

print(f"\n‚úÖ Queries de prueba creadas: {len(test_queries)}")
for i, q in enumerate(test_queries, 1):
    print(f"  {i}. '{q['query']}' ‚Üí {len(q['relevant_ids'])} documentos relevantes")

# Evaluar cada m√©todo
print("\n" + "=" * 80)
print("üîç EJECUTANDO EVALUACIONES")
print("=" * 80)

methods = ['hybrid', 'semantic', 'keyword']
k_values = [1, 3, 5, 10]

results = {}

for method in methods:
    print(f"\n{'=' * 80}")
    print(f"Evaluando m√©todo: {method.upper()}")
    print(f"{'=' * 80}")
    
    try:
        evaluation = evaluate_search_quality(
            test_queries=test_queries,
            method=method,
            k_values=k_values
        )
        
        results[method] = evaluation
        
        # Mostrar reporte
        report = format_evaluation_report(evaluation)
        print(report)
        
    except Exception as e:
        print(f"‚ùå Error evaluando {method}: {e}")
        import traceback
        traceback.print_exc()

# Comparaci√≥n final
print("\n" + "=" * 80)
print("üìä COMPARACI√ìN DE M√âTODOS")
print("=" * 80)

if results:
    print("\nPrecision@1:")
    for method, eval_data in results.items():
        p1 = eval_data['precision_at_k'].get(1, 0.0)
        print(f"  {method:10s}: {p1:.3f} ({p1*100:.1f}%)")
    
    print("\nMAP (Mean Average Precision):")
    for method, eval_data in results.items():
        map_score = eval_data['map']
        print(f"  {method:10s}: {map_score:.3f}")
    
    print("\nLatencia Media:")
    for method, eval_data in results.items():
        mean_lat = eval_data['latency_ms']['mean']
        status = "‚úÖ" if mean_lat < 200 else "‚ö†Ô∏è"
        print(f"  {method:10s}: {mean_lat:6.1f} ms {status}")
    
    print("\nRecall:")
    for method, eval_data in results.items():
        recall = eval_data['recall']
        print(f"  {method:10s}: {recall:.3f} ({recall*100:.1f}%)")
    
    # Mejor m√©todo por m√©trica
    print("\nüèÜ MEJORES M√âTODOS POR M√âTRICA:")
    
    best_p1 = max(results.items(), key=lambda x: x[1]['precision_at_k'].get(1, 0.0))
    print(f"  Precision@1: {best_p1[0]} ({best_p1[1]['precision_at_k'][1]:.1%})")
    
    best_recall = max(results.items(), key=lambda x: x[1]['recall'])
    print(f"  Recall:      {best_recall[0]} ({best_recall[1]['recall']:.1%})")
    
    best_latency = min(results.items(), key=lambda x: x[1]['latency_ms']['mean'])
    print(f"  Latencia:    {best_latency[0]} ({best_latency[1]['latency_ms']['mean']:.1f} ms)")
    
    best_map = max(results.items(), key=lambda x: x[1]['map'])
    print(f"  MAP:         {best_map[0]} ({best_map[1]['map']:.3f})")

print("\n" + "=" * 80)
print("‚úÖ EVALUACI√ìN COMPLETADA")
print("=" * 80)

# Recomendaciones
print("\nüí° RECOMENDACIONES:")
print("-" * 80)

if results:
    hybrid_eval = results.get('hybrid')
    if hybrid_eval:
        p1 = hybrid_eval['precision_at_k'].get(1, 0.0)
        lat = hybrid_eval['latency_ms']['mean']
        recall = hybrid_eval['recall']
        
        if p1 >= 0.95:
            print("‚úÖ Precision@1 excelente para b√∫squedas legales espec√≠ficas")
        else:
            print("‚ö†Ô∏è  Considera ajustar pesos RRF o mejorar embeddings para mayor Precision@1")
        
        if lat < 200:
            print("‚úÖ Latencia dentro del objetivo (<200ms)")
        else:
            print("‚ùå Latencia alta - considera:")
            print("   ‚Ä¢ Crear √≠ndice HNSW para embeddings")
            print("   ‚Ä¢ Optimizar par√°metros de b√∫squeda")
            print("   ‚Ä¢ Reducir top_k_candidates en RRF")
        
        if recall >= 0.80:
            print("‚úÖ Recall alto - el sistema encuentra la mayor√≠a de documentos relevantes")
        else:
            print("‚ö†Ô∏è  Recall bajo - el sistema puede estar perdiendo documentos relevantes")
            print("   ‚Ä¢ Aumentar top_k_candidates en b√∫squeda")
            print("   ‚Ä¢ Revisar calidad de embeddings")
            print("   ‚Ä¢ Verificar cobertura de search_vector")

print("\n" + "=" * 80)
